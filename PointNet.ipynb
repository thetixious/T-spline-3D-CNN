{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install trimesh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYd9QbIhT9G_",
        "outputId": "06939299-ffc2-4931-fad0-160daf47bcf3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trimesh\n",
            "  Downloading trimesh-4.6.8-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from trimesh) (2.0.2)\n",
            "Downloading trimesh-4.6.8-py3-none-any.whl (709 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/709.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m706.6/709.3 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m709.3/709.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trimesh\n",
            "Successfully installed trimesh-4.6.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMpLna_wkuQB",
        "outputId": "c7744e0f-a59f-4799-b060-983fc8e8aeb8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7J-Bwypa3HFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H54oJZRSTwwo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import trimesh\n",
        "from tqdm import tqdm\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CachedModelNet40Dataset(Dataset):\n",
        "    def __init__(self, root_dir, num_points=1024, split='train', cache_dir='cache'):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.num_points = num_points\n",
        "        self.cache_dir = os.path.join(root_dir, cache_dir, split)\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "        self.files = []\n",
        "        self.classes = []\n",
        "\n",
        "        for class_name in sorted(os.listdir(root_dir)):\n",
        "            class_path = os.path.join(root_dir, class_name)\n",
        "            split_path = os.path.join(class_path, split)\n",
        "            if not os.path.isdir(split_path):\n",
        "                continue\n",
        "\n",
        "            self.classes.append(class_name)\n",
        "\n",
        "            for file in os.listdir(split_path):\n",
        "                if file.endswith('.obj'):\n",
        "                    obj_path = os.path.join(split_path, file)\n",
        "                    npy_path = os.path.join(self.cache_dir, f\"{class_name}_{file}.npy\")\n",
        "                    self.files.append((obj_path, npy_path, class_name))\n",
        "\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(sorted(set(self.classes)))}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        obj_path, npy_path, class_name = self.files[idx]\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        if os.path.exists(npy_path):\n",
        "            points = np.load(npy_path)\n",
        "        else:\n",
        "            mesh = trimesh.load(obj_path, process=False)\n",
        "            points = np.array(mesh.vertices)\n",
        "            np.save(npy_path, points)\n",
        "\n",
        "        # ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğº num_points\n",
        "        if points.shape[0] < self.num_points:\n",
        "            diff = self.num_points - points.shape[0]\n",
        "            points = np.concatenate([points, points[np.random.choice(points.shape[0], diff)]])\n",
        "        else:\n",
        "            choice = np.random.choice(points.shape[0], self.num_points, replace=False)\n",
        "            points = points[choice]\n",
        "\n",
        "        # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ\n",
        "        points = points - np.mean(points, axis=0)\n",
        "        points = points / np.max(np.linalg.norm(points, axis=1))\n",
        "\n",
        "        return torch.from_numpy(points).float(), label\n"
      ],
      "metadata": {
        "id": "4RpSbvLcUKJH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TNet(nn.Module):\n",
        "    def __init__(self, k=3):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "\n",
        "        self.conv1 = nn.Conv1d(k, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k * k)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size(0)\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = torch.max(x, 2)[0]\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        identity = torch.eye(self.k, device=x.device).view(1, self.k * self.k).repeat(batchsize, 1)\n",
        "        x = x + identity\n",
        "        x = x.view(-1, self.k, self.k)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "4kqw4EcVUYei"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointNetClassifier(nn.Module):\n",
        "    def __init__(self, k=40):  # 40 ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ² ModelNet40\n",
        "        super().__init__()\n",
        "        self.input_transform = TNet(k=3)\n",
        "        self.feature_transform = TNet(k=64)\n",
        "\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k)\n",
        "\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, N, 3) â†’ (B, 3, N)\n",
        "        x = x.transpose(2, 1)\n",
        "        trans = self.input_transform(x)\n",
        "        x = torch.bmm(trans, x)\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        trans_feat = self.feature_transform(x)\n",
        "        x = torch.bmm(trans_feat, x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = torch.max(x, 2)[0]  # (B, 1024)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.dropout(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1), trans_feat\n"
      ],
      "metadata": {
        "id": "MA1ayYBKUa2F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_transform_regularizer(trans):\n",
        "    d = trans.size(1)\n",
        "    batchsize = trans.size(0)\n",
        "    I = torch.eye(d, device=trans.device).unsqueeze(0).repeat(batchsize, 1, 1)\n",
        "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2, 1)) - I, dim=(1, 2)))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "erfm34_cUkQO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for points, labels in tqdm(loader):\n",
        "        points, labels = points.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs, trans_feat = model(points)\n",
        "        loss = criterion(outputs, labels)\n",
        "        reg_loss = feature_transform_regularizer(trans_feat) * 0.001\n",
        "        total_batch_loss = loss + reg_loss\n",
        "\n",
        "        total_batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += total_batch_loss.item()\n",
        "        pred = outputs.max(1)[1]\n",
        "        correct += pred.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n"
      ],
      "metadata": {
        "id": "lY8aFu-hUlYT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for points, labels in loader:\n",
        "            points, labels = points.to(device), labels.to(device)\n",
        "            outputs, _ = model(points)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = outputs.max(1)[1]\n",
        "            correct += pred.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n"
      ],
      "metadata": {
        "id": "K3nzAqseUoR0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸\n",
        "BATCH_SIZE = 32\n",
        "NUM_POINTS = 1024\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…\n",
        "train_dataset = CachedModelNet40Dataset(root_dir='/content/drive/MyDrive/PointNet/ModelNet40', num_points=NUM_POINTS, split='train')\n",
        "test_dataset = CachedModelNet40Dataset(root_dir='/content/drive/MyDrive/PointNet/ModelNet40', num_points=NUM_POINTS, split='test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PointNetClassifier(k=len(train_dataset.class_to_idx)).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.NLLLoss()\n"
      ],
      "metadata": {
        "id": "E-HQt51vUrQG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"  Test  Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThO_S__HVm__",
        "outputId": "6289a020-90ff-4cc9-fdc3-db3281d1e1f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [36:13<00:00,  7.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "  Train Loss: 2.2278, Accuracy: 0.4338\n",
            "  Test  Loss: 1.9113, Accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:55<00:00,  5.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15\n",
            "  Train Loss: 1.5686, Accuracy: 0.5759\n",
            "  Test  Loss: 1.3521, Accuracy: 0.6305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:41<00:00,  7.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15\n",
            "  Train Loss: 1.2553, Accuracy: 0.6559\n",
            "  Test  Loss: 1.1461, Accuracy: 0.6754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15\n",
            "  Train Loss: 1.0931, Accuracy: 0.6910\n",
            "  Test  Loss: 1.0523, Accuracy: 0.7075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15\n",
            "  Train Loss: 0.9336, Accuracy: 0.7276\n",
            "  Test  Loss: 0.9059, Accuracy: 0.7277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15\n",
            "  Train Loss: 0.8217, Accuracy: 0.7607\n",
            "  Test  Loss: 0.7957, Accuracy: 0.7622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/15\n",
            "  Train Loss: 0.7536, Accuracy: 0.7812\n",
            "  Test  Loss: 0.7548, Accuracy: 0.7699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:41<00:00,  7.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/15\n",
            "  Train Loss: 0.7209, Accuracy: 0.7843\n",
            "  Test  Loss: 0.7714, Accuracy: 0.7739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/15\n",
            "  Train Loss: 0.6510, Accuracy: 0.8042\n",
            "  Test  Loss: 0.6946, Accuracy: 0.7954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/15\n",
            "  Train Loss: 0.6407, Accuracy: 0.8071\n",
            "  Test  Loss: 0.7730, Accuracy: 0.7759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15\n",
            "  Train Loss: 0.6104, Accuracy: 0.8119\n",
            "  Test  Loss: 0.7184, Accuracy: 0.7857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/15\n",
            "  Train Loss: 0.5572, Accuracy: 0.8304\n",
            "  Test  Loss: 0.6639, Accuracy: 0.8031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/15\n",
            "  Train Loss: 0.5382, Accuracy: 0.8329\n",
            "  Test  Loss: 0.6644, Accuracy: 0.7954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/15\n",
            "  Train Loss: 0.5506, Accuracy: 0.8270\n",
            "  Test  Loss: 0.6697, Accuracy: 0.8019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:40<00:00,  7.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/15\n",
            "  Train Loss: 0.4961, Accuracy: 0.8433\n",
            "  Test  Loss: 0.6871, Accuracy: 0.8023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'last_pointnet_model.pth')\n",
        "print(\"ğŸ“¦ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LgX5BUT1lxv",
        "outputId": "db7518d9-44c5-47f3-b19c-fd34df4e4b3b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.\n"
          ]
        }
      ]
    }
  ]
}